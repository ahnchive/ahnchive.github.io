---
layout: frontpage
title: seoyoung ahn
description: Homepage for Seoyoung (Young) Ahn, PhD student at Stony Brook University 
keywords: Seoyoung Ahn, Vision, Attention, Eye Tracking, Deep Learning
---


<div class="container">
<h4><a name="profile"></a>Seoyoung Ahn</h4>

    <div class="row-fluid">
        
        <div class="span2">
        <a href="../assets/squirrel.png">
            <img src="../assets/squirrel.png" align="center" width="400px" style="margin-bottom: 20px"
                  title="headshot" alt="headshot"/></a>    
        </div>
        
        <div class="span5">
            PhD student<br/>
            Stony Brook University, NY<br/>
            seoyoung.ahn@stonybrook.edu<br/><br/><br/>

            <a href="https://scholar.google.com/citations?user=FxF4Y1UAAAAJ&hl=en">google scholar</a><br/>
            <a href="https://twitter.com/seoyahn">twitter (@seoyahn)</a><br/>
            <a href="https://github.com/ahnchive">github</a><br/>
            
        </div>


    </div>
</div>

&nbsp;
&nbsp;
### About
I'm a PhD student at [EyeCog lab](https://you.stonybrook.edu/zelinsky/who-we-are/) at Stony Brook University. I'm broadly interested in understanding how humans can obtain stable but flexible representations of their visual environment. I try to understand this question by building an artificial vision system that is more human-like. Please find my curriculum vitae [here]({{ BASE_PATH }}/assets/CV.pdf) (updated 11/04/2022)


&nbsp;
&nbsp;
### Recent News

My second-year project "Use of superordinate labels yields more robust and human-like visual representations in convolutional neural networks" has been accepted at [Journal of Vision](https://jov.arvojournals.org/article.aspx?articleid=2778207)!

Gave a live talk at [VSS 2020](https://jov.arvojournals.org/article.aspx?articleid=2771677) on how hieararchical semantic structure of the training labels helps visual category learning in convolutional neural networks

We published a large-scale eyetracking dataset for training deep learning models. See our project page: [COCO-Search18](https://sites.google.com/view/cocosearch/)

My first-year project "Towards Predicting Reading Comprehension From Gaze Behavior" has been accepted at [ETRA 2020](https://dl.acm.org/doi/10.1145/3379156.3391335)

Our team presented our real-time reading vs skimming detector at [ETRA 2019](https://dl.acm.org/doi/10.1145/3314111.3319916)


<!-- <div style="text-align: justify"> I'm broadly interested in understanding how humans can obtain a stable but flexible representation of the visual environment. I try to understand this question by building an artificial vision system that is more human-like! </div>
&nbsp; -->



<!-- <p align="center">
  <img src="{{ BASE_PATH }}/assets/drawing.jpg" align="center" width="500px" style="margin-bottom: 20px; margin-top: 20px"/>
</p>
 -->

<!-- ![main figure]({{ BASE_PATH }}/assets/drawing.jpg){:height="50%" width="50%"; style="float: left" } -->
<!-- ![main figure]({{ BASE_PATH }}/assets/drawing.jpg){:height="100%"} -->

<!-- <br clear="left"/> -->
<!-- <img src="{{ BASE_PATH }}/assets/drawing.jpg" style="margin-bottom: 10px; margin-top: 10px"/> -->






<!-- 
&nbsp;
<div class="navbar">
  <div class="navbar-inner">
      <ul class="nav">
          <li><a href="{{ BASE_PATH }}/assets/CV.pdf">cv</a></li>
          <li><a href="https://github.com/ahnchive">github</a></li>
          <li><a href="https://twitter.com/seoyahn">twitter (@seoyahn)</a></li>
          <li><a href="https://scholar.google.com/citations?user=FxF4Y1UAAAAJ&hl=en">google scholar</a></li>
      </ul>
  </div>
</div> -->



