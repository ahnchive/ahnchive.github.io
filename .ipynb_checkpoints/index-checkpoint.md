---
layout: frontpage
title: seoyoung ahn
description: Homepage for Seoyoung (Young) Ahn, PhD student at Stony Brook University 
keywords: Seoyoung Ahn, Vision, Attention, Eye Tracking, Deep Learning
---


<div class="container">
<h4><a name="profile"></a>Seoyoung Ahn</h4>

    <div class="row-fluid">
        
        <div class="span2">
        <a href="../assets/squirrel.png">
            <img src="../assets/squirrel.png" align="center" width="400px" style="margin-bottom: 20px"
                  title="headshot" alt="headshot"/></a>    
        </div>
        
        <div class="span5">
            PhD student<br/>
            Stony Brook University, NY<br/>
            seoyoung.ahn@stonybrook.edu<br/><br/><br/>

            <a href="https://scholar.google.com/citations?user=FxF4Y1UAAAAJ&hl=en">google scholar</a><br/>
            <a href="https://twitter.com/seoyahn">twitter (@seoyahn)</a><br/>
            <a href="https://github.com/ahnchive">github</a><br/>
            
        </div>


    </div>
</div>

&nbsp;
&nbsp;
### About
I'm a PhD student at [EyeCog lab](https://you.stonybrook.edu/zelinsky/who-we-are/) at Stony Brook University. I'm broadly interested in understanding how humans can obtain stable but flexible representations of their visual environment. I try to understand this question by building an artificial vision system that is more human-like. Please find my curriculum vitae [here]({{ BASE_PATH }}/assets/CV.pdf) (updated 11/04/2022)


&nbsp;
&nbsp;
### Recent News
Our paper "Reconstruction-guided attention improves the robustness and shape processing of neural networks" has been accepted to [SVRHM Workshop](https://openreview.net/forum?id=tmvg0VIHTDr&noteId=u48ShDUHey) at Neurips 2022. I will be there in person to present my poster.

Our IRL model now can predict human search behavior when the target is not there. Accepted at [ECCV 2022](https://arxiv.org/abs/2207.01166)

Gave a talk at [MODVIS 2022](https://www.purdue.edu/conferences/events/modvis/index.php), VSS workshop on Computational and Mathematical Models in Vision. Really enjoyed meeting new colleagues and dicussing interesting work!

I will be giving a talk at [NAISys 2022](https://meetings.cshl.edu/abstracts.aspx?meet=NAISYS&year=22) about how top-down object reconstruction improves the model's recognition robustness. Very excited for the first in-person conferene in years after Covid break!

My second-year project "Use of superordinate labels yields more robust and human-like visual representations in convolutional neural networks" has been accepted at [Journal of Vision](https://jov.arvojournals.org/article.aspx?articleid=2778207)!

Gave a live talk at [VSS 2020](https://jov.arvojournals.org/article.aspx?articleid=2771677) on how hieararchical semantic structure of the training labels helps visual category learning in convolutional neural networks

We published a large-scale search eyetracking dataset for training deep learning models. See our project page: [COCO-Search18](https://sites.google.com/view/cocosearch/)

Our [GazePrediction](https://ai.stonybrook.edu/about-us/News/Eye-catching-12M-NSF-award-CS-and-Psychology-Researchers) team published our first paper at [CVPR 2020](https://openaccess.thecvf.com/content_CVPR_2020/papers/Yang_Predicting_Goal-Directed_Human_Attention_Using_Inverse_Reinforcement_Learning_CVPR_2020_paper.pdf) and received best paper nomination. We used inverse reinforcement learning (IRL) to model human goal-directed attention.

My first-year project "Towards Predicting Reading Comprehension From Gaze Behavior" has been accepted at [ETRA 2020](https://dl.acm.org/doi/10.1145/3379156.3391335)

Our [WebGaze](https://www3.cs.stonybrook.edu/~arunab/gaze/index.html) team presented our real-time reading vs skimming detector at [ETRA 2019](https://dl.acm.org/doi/10.1145/3314111.3319916)


<!-- <div style="text-align: justify"> I'm broadly interested in understanding how humans can obtain a stable but flexible representation of the visual environment. I try to understand this question by building an artificial vision system that is more human-like! </div>
&nbsp; -->



<!-- <p align="center">
  <img src="{{ BASE_PATH }}/assets/drawing.jpg" align="center" width="500px" style="margin-bottom: 20px; margin-top: 20px"/>
</p>
 -->

<!-- ![main figure]({{ BASE_PATH }}/assets/drawing.jpg){:height="50%" width="50%"; style="float: left" } -->
<!-- ![main figure]({{ BASE_PATH }}/assets/drawing.jpg){:height="100%"} -->

<!-- <br clear="left"/> -->
<!-- <img src="{{ BASE_PATH }}/assets/drawing.jpg" style="margin-bottom: 10px; margin-top: 10px"/> -->






<!-- 
&nbsp;
<div class="navbar">
  <div class="navbar-inner">
      <ul class="nav">
          <li><a href="{{ BASE_PATH }}/assets/CV.pdf">cv</a></li>
          <li><a href="https://github.com/ahnchive">github</a></li>
          <li><a href="https://twitter.com/seoyahn">twitter (@seoyahn)</a></li>
          <li><a href="https://scholar.google.com/citations?user=FxF4Y1UAAAAJ&hl=en">google scholar</a></li>
      </ul>
  </div>
</div> -->



