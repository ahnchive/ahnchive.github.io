---
layout: page
title: The research projects that I'm currently working on:
description: project
---


## The role of top-down object reconstruction in object perception
Humans need to interact with objects, so evolution endowed us with a visual system that constantly attempts to reconstruct familar or meaningful objects; the proverbial bunny in a cloud! This project explores how and to what extent this top-down object reconstruction is functionally used for object recognition, grouping, and attention. We use a generative, object-centric approach to study this problem. 

<p align="center">
  <img src="{{ BASE_PATH }}/pages/figures/recon-mnist.jpeg" align="center" style="margin-bottom: 20px; margin-top: 20px"/>
</p>


### Related Publications:
- Ahn S, Adeli H, Zelinsky GJ. Reconstruction-guided attention improves the robustness and shape processing of neural networks. Advances in Neural Information Processing Systems Wokshops (SVRHM at Neurips Workshops). 2022
- Adeli H, Ahn S, Zelinsky G. A brain-inspired object-based attention network for multi-object recognition and visual reasoning. bioRxiv. 2022 (under review)
- Adeli H, Ahn S, Zelinsky G. Recurrent attention models with object-centric capsule representation for multiobject recognition. arXiv preprint arXiv:2110.04954. 2021


## The effects of language on human visual perception 
The visual experience is not the only source of learning for humans. We constantly read and listen about things around us, and we actively use this prior knowledge to perceive our visual surroundings. This project investigates how language affects human visual perception and attention control. We build neural networks that leverage both language and visual information and compare their performance with human behavioral and neural data under the same visual tasks. 

### Related Publications:
- Ahn S, Zelinsky GJ, Lupyan G. Use of superordinate labels yields more robust and human-like visual representations in convolutional neural networks. Journal of vision. 2021


## Human attention modeling and prediction
How humans allocate their spatial attention is a question having clear benefits to both computer and behavioral vision (e.g., the development of next-generation systems that could intelligently anticipate a user's needs or desires). This project attempts to reverse-engineer what visual features attract human spatial attention ("priority maps") and build the computional model to predict human eye-movements during various visual tasks. Interested in training your models to predict human attention? You can download the dataset here [COCO-Search18](https://sites.google.com/view/cocosearch/)) for categorical search and [COCO-Freeview](https://sites.google.com/view/cocosearch/coco-freeview?authuser=0) for freeview. 

<p align="center">
  <img src="{{ BASE_PATH }}/pages/figures/irl.png" align="center" style="margin-bottom: 20px; margin-top: 20px"/>
</p>


### Related Publications:
- Yang Z, Mondal S, Ahn S, Zelinsky G, Hoai M, Samaras D. Target-absent Human Attention. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (ECCV). 2022
- Chen Y, Yang Z, Chakraborty S, Mondal S, Ahn S, Samaras D, Hoai M, Zelinsky G. Characterizing Target Absent Human Attention. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (ECCV Workshops). 2022
- Chakraborty S, Wei Z, Kelton C, Ahn S, Balasubramanian A, Zelinsky GJ, Samaras D. Predicting visual attention in graphic design documents. IEEE Transactions on Multimedia. 2022
- Chen Y, Yang Z, Ahn S, Samaras D, Hoai M, Zelinsky G. COCO-Search18 fixation dataset for predicting goal-directed attention control. Scientific reports. 2021
- Zelinsky G, Chen Y, Ahn S, Adeli H, Yang Z, Huang L, Samaras D, Hoai M. Predicting goal-directed attention control using inverse-reinforcement learning. Neurons, behavior, data analysis and theory. 2021
- Yang Z, Huang L, Chen Y, Wei Z, Ahn S, Zelinsky G, Samaras D, Hoai M. Predicting goal-directed human attention using inverse reinforcement learning. Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (CVPR). 2020
- Zelinsky G, Yang Z, Huang L, Chen Y, Ahn S, Wei Z, Adeli H, Samaras D, Hoai M. Benchmarking gaze prediction for categorical visual search. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPR Workshops). 2019


## Decoding your mind from eye movemements
Can we decode someone's mental states just from their eye movements? This project attempt to implement deep learning or machin learning models to predict one's mental or cognitive states (e.g., task type, comprehension, engagement, mind wandering) from eye movements. 

<p align="center">
  <img src="{{ BASE_PATH }}/pages/figures/comprehension.jpeg" align="center" style="margin-bottom: 20px; margin-top: 20px"/>
</p>

### Related Publications:
- Ahn S, Lee D, Hinojosa A, Koh S. Task Effects on Perceptual Span during Reading: Evidence from Eye Movements in Scanning, Proofreading, and Reading for Comprehension (under review)
- Ahn S, Kelton C, Balasubramanian A, Zelinsky G. Towards predicting reading comprehension from gaze behavior. ACM Symposium on Eye Tracking Research and Applications (ETRA). 2020
- Kelton C, Wei Z, Ahn S, Balasubramanian A, Das SR, Samaras D, Zelinsky G. Reading detection in realtime. Proceedings of the 11th ACM Symposium on Eye Tracking Research & Applications (ETRA). 2019



<!-- [click here for the most recent version of the paper]({{ BASE_PATH}}/pages/working_papers/sample-working-paper.pdf) -->


<!-- Note: this is how to write a comment in HTML. Everything in here won't show up on your webpage.-->

<!--
To increase the size of the title, use fewer # in front of the paper title.
To decrease the size of the title, use more #. 
To remove the italics, remove the * before and after the description
To remove the underline from the title, remove the <u> tags (<u> and </u>)
-->
