---
layout: frontpage
title: Seo-Young Ahn's Homepage
description: Academic Website for Seoyoung (Young) Ahn
keywords: vision, attention, eye tracking, deep learning
---


<div class="container">
    <div class="row-fluid">
        
        <div class="span2">
          <img src="{{ BASE_PATH }}/pics/vss_profile.jpeg"
                title="Seoyoung Ahn" alt="Seoyoung Ahn"/>
        </div>

        <div class="span5">
          <h4><a name="Seo-Young Ahn"></a>Seo-Young Ahn</h4>

<!--           final-year PhD student @<a href="https://you.stonybrook.edu/zelinsky/who-we-are/" target="_blank">eyecog lab</a><br/>
          Stony Brook University, New York, USA <br/><br/> -->
          postdoc @<a href="https://tsaolab.berkeley.edu/" target="_blank">Tsao lab</a><br/>
          UC Berkeley, California, USA <br/><br/>

          CV: <a href="{{ BASE_PATH }}/ahn_cv.pdf"  target="_blank"><img src="pics/icons16/pdf-icon.png" alt="CV as pdf"></a> (updated May 2024)

          <div id="hide_email">
          email: <code>ahn</code><b>I</b><code>seoyoung</code><b>don't</b><code>@</code><b>want</b><code></code><b>spam!
          </b><code></code><b>So</b><code></code><b>please</b><code>gma</code><b>leave
          </b><code>il</code><b>me</b><code>.</code><b>alone</b><code>c</code><b>!</b><code>om</code><br/>
            </div>
          social: <a href="https://twitter.com/seoyahn" target="_blank">twitter</a>, <a href="https://scholar.google.com/citations?user=FxF4Y1UAAAAJ&hl=en" target="_blank">google scholar</a><br/><br/>

        
        </div>


    </div>
</div>


<!-- <h5><a name="About"></a>ABOUT</h5>
I’m broadly interested in understanding how humans can obtain stable but flexible representations of their visual environment. I try to understand this question by building an artificial vision system that is more human-like!<br/><br/> -->

<!-- <h5><a name="Recent News"></a>RECENT NEWS</h5>

I was awarded a fellowship from the Stony Brook University Endowed Award Fund for Cognitive Science. This award will greatly help my phd disseration research!<br/>

Our paper "Reconstruction-guided attention improves the robustness and shape processing of neural networks" has been accepted to <a href="https://openreview.net/forum?id=tmvg0VIHTDr&noteId=u48ShDUHey">SVRHM workshop</a> at Neurips 2022. I will be there in person to present my poster.<br/>

Our IRL model now can predict human search behavior when the target is not there. Accepted at <a href="https://arxiv.org/abs/2207.01166">ECCV</a><br/>

Gave a talk at <a href="https://www.purdue.edu/conferences/events/modvis/index.php">MODVIS</a>, VSS workshop on Computational and Mathematical Models in Vision. Really enjoyed meeting new colleagues and dicussing interesting work!<br/>

I will be giving a talk at <a href="https://meetings.cshl.edu/abstracts.aspx?meet=NAISYS&year=22">NAISys 2022</a> about how top-down object reconstruction improves the model's recognition robustness. Very excited for the first in-person conferene in years after Covid break!<br/>

My second-year project "Use of superordinate labels yields more robust and human-like visual representations in convolutional neural networks" has been accepted at <a href="https://jov.arvojournals.org/article.aspx?articleid=2778207">Journal of Vision</a><br/>

Gave a live talk at <a href="https://jov.arvojournals.org/article.aspx?articleid=2771677">VSS 2020</a> on how hieararchical semantic structure of the training labels helps visual category learning in convolutional neural networks<br/>

We published a large-scale search eyetracking dataset for training deep learning models. See our project page: <a href="https://sites.google.com/view/cocosearch/">COCO-Search18</a><br/>

Our <a href="https://ai.stonybrook.edu/about-us/News/Eye-catching-12M-NSF-award-CS-and-Psychology-Researchers">GazePrediction</a> team published our first paper at <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Yang_Predicting_Goal-Directed_Human_Attention_Using_Inverse_Reinforcement_Learning_CVPR_2020_paper.pdf">CVPR</a> and received best paper nomination. We used inverse reinforcement learning (IRL) to model human goal-directed attention.<br/>

My first-year project "Towards Predicting Reading Comprehension From Gaze Behavior" has been accepted at <a href="https://arxiv.org/abs/2207.01166">ECCV</a>[ETRA 2020](https://dl.acm.org/doi/10.1145/3379156.3391335)<br/>

Our <a href="https://www3.cs.stonybrook.edu/~arunab/gaze/index.html">WebGaze</a> team presented our real-time reading vs skimming detector at <a href="https://dl.acm.org/doi/10.1145/3314111.3319916">ETRA 2019</a><br/> -->



---

##### ABOUT
I'm a postdoc working at [Doris Tsao](https://tsaolab.berkeley.edu/){:target="_blank"}'s lab at UC Berkeley. I did my PhD with Dr. [Greg Zelinsky](https://you.stonybrook.edu/zelinsky/who-we-are/){:target="_blank"} at Stony Brook University. I also frequently collaborate with the Computer Vision lab with Dr. [Dimitris Samaras](https://www3.cs.stonybrook.edu/~samaras/){:target="_blank"} and Dr. [Minh Hoai Nguyen](https://www3.cs.stonybrook.edu/~minhhoai/){:target="_blank"}. Before that, I did my MA and BA at Seoul National University, and studied the computational omdeling of eye-movements with Dr. Sungryong Koh. I’m broadly interested in understanding how humans can obtain stable but flexible representations of their visual environment. I’m currently exploring the use of generative models to explain human object-based perception and attention, and understanding how the brain accomplishes this. In my free time, I write for [animated film](https://jinukchoi.com/hairuniverse)!


<br/>

##### RECENT NEWS

I'm co-organizing the ['Using Deep Networks to Re-imagine Object-based Attention and Perception'](https://www.visionsciences.org/symposia/?sym=21){:target="_blank"} symposium at VSS 2024. We will discuss topics ranging from classic object-based attention theories and recent neural and behavioral data to new deep learning models for better understanding how the visual system forms meaningful, coherent object percepts.

I'm moving to UC Berkeley to start a new postdoc position at Doris Tsao's lab in 2024. I'm excited to begin a new journey studying monkey brain!

The 3D [animated film](https://jinukchoi.com/hairuniverse) I participated as a writer won the Jury's Special Award at SIGGRAPH Asia 2023! Huge congratulations to our amazing director, Jinuk! 

I am honored to receive the 2023 APA Disseration Research Award!

We presented several [talks](https://www.visionsciences.org/presentation/?id=5129) and [posters](https://www.visionsciences.org/presentation/?id=5978) at this year's VSS 2023 on modeling human object-based attention. 

I received a travel award from Females of Vision et al. ([Fovea](http://www.foveavision.org/))

I was awarded the Endowed Award Fund for Cognitive Science from Stony Brook Psychology!

Our paper "Reconstruction-guided attention improves the robustness and shape processing of neural networks" has been accepted to [SVRHM Workshop](https://openreview.net/forum?id=tmvg0VIHTDr&noteId=u48ShDUHey){:target="_blank"} at Neurips 2022. I will be there in person to present my poster.

Our IRL model now can predict human search behavior when the target is not there. Accepted at [ECCV 2022](https://arxiv.org/abs/2207.01166){:target="_blank"}

Gave a talk at [MODVIS 2022](https://www.purdue.edu/conferences/events/modvis/index.php){:target="_blank"}, VSS workshop on Computational and Mathematical Models in Vision. Really enjoyed meeting new colleagues and dicussing interesting work!

I will be giving a talk at [NAISys 2022](https://meetings.cshl.edu/abstracts.aspx?meet=NAISYS&year=22){:target="_blank"} about how top-down object reconstruction improves the model's recognition robustness. Very excited for the first in-person conferene in years after Covid break!

My second-year project "Use of superordinate labels yields more robust and human-like visual representations in convolutional neural networks" has been accepted at [Journal of Vision](https://jov.arvojournals.org/article.aspx?articleid=2778207){:target="_blank"}

Gave a live talk at [VSS 2020](https://jov.arvojournals.org/article.aspx?articleid=2771677){:target="_blank"} on how hieararchical semantic structure of the training labels helps visual category learning in convolutional neural networks

We published a large-scale search eyetracking dataset for training deep learning models. See our project page: [COCO-Search18](https://sites.google.com/view/cocosearch/){:target="_blank"}

Our [GazePrediction](https://ai.stonybrook.edu/about-us/News/Eye-catching-12M-NSF-award-CS-and-Psychology-Researchers){:target="_blank"} team published our first paper at [CVPR 2020](https://openaccess.thecvf.com/content_CVPR_2020/papers/Yang_Predicting_Goal-Directed_Human_Attention_Using_Inverse_Reinforcement_Learning_CVPR_2020_paper.pdf){:target="_blank"} and received best paper nomination. We used inverse reinforcement learning (IRL) to model human goal-directed attention.

My first-year project "Towards Predicting Reading Comprehension From Gaze Behavior" has been accepted at [ETRA 2020](https://dl.acm.org/doi/10.1145/3379156.3391335){:target="_blank"}

Our [WebGaze](https://www3.cs.stonybrook.edu/~arunab/gaze/index.html) team presented our real-time reading vs skimming detector at [ETRA 2019](https://dl.acm.org/doi/10.1145/3314111.3319916){:target="_blank"}


<br/>
<div class="navbar-centered">
  <div class="navbar-inner-centered">
      <ul class="nav">
          <li><a href="{{ BASE_PATH }}/pages/morenews.html">more news</a></li>
      </ul>
  </div>
</div>




